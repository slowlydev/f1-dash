/* eslint-disable camelcase */
// from https://github.com/xenova/whisper-web/blob/main/src/worker.js
import { pipeline, env } from "@xenova/transformers";

// Disable local models
env.allowLocalModels = false;

// Define model factories
// Ensures only one model is created of each type
class PipelineFactory {
	static task = null;
	static model = null;
	static quantized = null;
	static instance = null;

	constructor(model, quantized) {
		this.model = model;
		this.quantized = quantized;
	}

	static async getInstance(progress_callback = null) {
		if (this.instance === null) {
			this.instance = pipeline(this.task, this.model, {
				quantized: this.quantized,
				progress_callback,
				// For medium models, we need to load the `no_attentions` revision to avoid running out of memory
				revision: this.model.includes("/whisper-medium") ? "no_attentions" : "main",
			});
		}

		return this.instance;
	}
}

self.addEventListener("message", async (event) => {
	const message = event.data;

	// Do some work...
	// TODO use message data
	let transcript;
	try {
		transcript = await transcribe(
			message.audio,
			message.model,
			message.multilingual,
			message.quantized,
			message.subtask,
			message.language,
		);
	} catch (e) {
		console.warn("Error while transcribing: " + e);
		transcript = {
			text: "",
			chunks: [],
		};
	}

	// Send the result back to the main thread
	self.postMessage({
		status: "complete",
		task: "automatic-speech-recognition",
		key: message.key,
		data: transcript,
	});
});

class AutomaticSpeechRecognitionPipelineFactory extends PipelineFactory {
	static task = "automatic-speech-recognition";
	static model = null;
	static quantized = null;
}

const transcribe = async (audio, model, multilingual, quantized, subtask, language) => {
	const isDistilWhisper = model.startsWith("distil-whisper/");

	let modelName = model;
	if (!isDistilWhisper && !multilingual) {
		modelName += ".en";
	}

	const p = AutomaticSpeechRecognitionPipelineFactory;
	if (p.model !== modelName || p.quantized !== quantized) {
		// Invalidate model if different
		p.model = modelName;
		p.quantized = quantized;

		if (p.instance !== null) {
			(await p.getInstance()).dispose();
			p.instance = null;
		}
	}

	// Load transcriber model
	let transcriber = await p.getInstance((data) => {
		self.postMessage(data);
	});

	// Actually run transcription
	let output = await transcriber(audio, {
		// Greedy
		top_k: 0,
		do_sample: false,

		// Sliding window
		chunk_length_s: isDistilWhisper ? 20 : 30,
		stride_length_s: isDistilWhisper ? 3 : 5,

		// Language and task
		language: language,
		task: subtask,

		// Return timestamps
		return_timestamps: true,
		force_full_sequences: false,
	});

	return output;
};
